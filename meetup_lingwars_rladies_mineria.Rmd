---
title: "R-Ladies + Lingwars: Taller de minería de textos con R"
subtitle: "R-Ladies Madrid, Verónica García y Claudia Guirao, 17/10/2017"
output: 
  html_notebook: 
    fig_caption: yes
    fig_width: 8
    toc: yes
---
## ¿Quiénes somos?

- **Verónica García**, @yryaa 
- **Claudia Guirao**, @claudiaguirao 

RLadies y Data Scientists en **Kernel Analytics** 

Código y datos en https://github.com/intiveda/rladies_textmining 

## Text Mining & NLP

Gran parte de los datos se encuentran no estructurados, es importante conocer técnicas que nos permitan obtener conclusiones a partir de los mensajes que generan nuestras organizaciones, clientes o usuarios. 

Hoy aprenderemos algunas técnicas básicas para manipular cadenas de texto y aplicaremos técnicas de NLP a subtítulos para obtener algunas conclusiones. 

## Materiales y librerías 

* Subtítulos temporadas de los Simpsons
* Librerías:
 + ```stringr```
 + ```subtools```
 + ```tm```
 + ```tidytext```
 + ```tidyverse``` 
 + ```dplyr```
 + ```datatable```
 + ```ggplot2```
 + ```plotly```(opcional)
 + ```igraph```
 + ```ggraph```
 + ```worldcloud```
 + ```knitr```
 + ```wordcloud```

Puedes seguir este tutorial de dos formas:

1. Escuchando activamente, para probar luego en casa :)
2. De forma activa, ejecutando en tu equipo:
* Descarga el notebook del repositorio. 
* Abre el archivo ```.Rmd``` en R Studio (para poder ejecutar notebooks necesitarás algunas dependencias)
* Fija el __working directory__ dentro de la carpeta ```rladies_textmining```: ```setwd("eldirectoriodondehasdescargadoelrepo/rladies_textmining/")```
* Chequea que tienes las librerías instaladas. 

```{r, echo=TRUE, warning=TRUE}
c(
  "stringr",
  "subtools",
  "tm",
  "tidytext",
  "tidyverse",
  "dplyr",
  "data.table",
  "ggplot2",
  "plotly",
  "igraph",
  "ggraph",
  "wordcloud",
  "knitr"
  ) %in% rownames(installed.packages())
```

***

## Back2basics: Strings y expresiones regulares

Las cadenas o strings cumplen un papel importante en las tareas de ETL o preparación de los datos. Una de las librerías esenciales en la materia es **stringr**  

```Stringr``` es uno de los paquetes diseñados por Hadley Wickham para asistir en las tareas de manipulación de strings:

+ se integra con _pipes_ (%<%)
+ sus funciones son consistentes y fáciles de interpretar

### ¿Qué son las cadenas de texto o _strings_?

+ Las cadenas de texto se encuentran contenidas entre comillas `""` (*usar la comilla simple `'` para escapar la doble comilla)
+ Pueden contener letras `"a"`, números `"1"`, símbolos `"&"` o todo lo anterior `"1a&"`
+ Mientras que los números puede ser a la vez _integers_ y _characters_, las letras y los símbolos no tienen significado como integer y se traducen en `NA`

```{r as.integer, warning=FALSE}
as.integer(c("a", "&", "123"))
```

```{r list1}
c(factor("a"), "b", "&",1)
```
Concatenar integers y characters, convierte automáticamente los integers en _characters_.   

```{r list2}
c(as.character(factor("a")), "b", "&",1)
```

### Operaciones básicas

#### Importar la librería
```{r imports, warning=FALSE}
# install.packages("stringr")
library(stringr)
```

#### Operadores

Muchas de estas funciones tiene su equivalente en R base, pueden ser más lentas/menos eficientes.

+ `str_to_upper(string)`: convierte un string en mayúsculas
+ `str_to_lower(string)`: convierte un string en minúsculas
+ `str_to_title(string)`: capitaliza un string


```{r temas}
temas <- c("Código", "Mujeres", "tecnología", "Informática", "estadística", "Women", "Coders", "Aprendizaje", "automático", "Análisis", "datos", "Visualización", "R-Ladies", "Social", "Coding", "R", "Ciencia", "Programming")
```

```{r mayus}
str_to_upper(temas)
```
```{r minus}
str_to_lower(temas)
```
```{r capi}
str_to_title(temas)
```

+ `str_c(string, sep = "")`: junta varios string en uno solo, es el equivalente a `paste(sep = "")` o `paste0()`
+ `str_length(string)`: devuelve la longitud del string, es similar a la función `nchar()`. Convierte los factores en strings y conserva los NA's

```{r length}
print(str_length('R-Ladies'))
print(str_length(NA))
```

+ `str_sub(string, start, end)`: subsetea un string o un vector de string especificando la posición inicial y la final, es el equivalente en R base a substr(). Por defecto finaliza en el último caracter.

```{r subsetting2}
print(temas[1:4])
str_sub(string = temas[1:4], start=3)
```
+ `str_dup(string, times)`: copia y pega un string un número determinado de veces
```{r duplicating}
str_dup(string = temas[1:4], times = 3)
```

+ `str_trim(string, side = c("both", "left", "rigth"))`: elimina los espacios vacíos, por defecto toma el valor _both_. Mejor evitar `gsub(" ", "", string)`
+ `str_pad(string, width, side = c("left", "both", "right"), pad = " "))`: añade a strings espacios en blanco para igualarlos en longitud, especialmente útil para añadir 0 a números. 

### Expresiones regulares

Las expresiones regulares ( _regular expressions_, _regex_, _pattern matching_) son un lenguaje usado para parsear y manipular texto. Se usan comúnmente para hacer operaciones de búsqueda y reemplazo y para validar si un texto está bien formado.  

##### Expresiones comunes

- "a"  = letra "a"
- "^a" = empieza con la letra "a"
- "a$" = finaliza con la letra "a"
- "[ ]" = contiene cualquier letra (o número) de las contenidas en los corchetes
- "[ - ]" = contiene cualquier letra (o número) dentro de un rango
- "[^ae]" = cualquier cosa excepto determinadas letras (o números)
- "{3}" = repite la expresión regular 3 veces  

Las expresiones regulares son un mundo en si mismo, aquí tienes una pequeña _chuleta_ : https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf   


```{r regex}
rcosas = c("baseR", "R-Ladies", "Rmeetup", "Rmarkdown", "stringR")
str_detect(rcosas, pattern = "^R")
```
```{r regex2}
rcosas[str_detect(rcosas, pattern = "^R")]
```

```{r regex3}
rcosas[str_detect(rcosas, pattern = "R")]
```
```{r regex4}
cleanR <- c("tidyverse", "tidyr","dplyr", "ggplot2", "tidytext", "purrr")
str_locate(cleanR, "tidy")
```

##### Otras funciones 
+ `str_extract(string, pattern)` o `str_extract_all()`: busca la palabra exacta (normalmente se utiliza con expresiones regulares concatenadas)
+ `str_match(string, pattern)` o `str_match_all()`: es una función equivalente pero devuelve una matriz

```{r regex5}
str_match(c("12345678", "12587465", "dni desconocido"), pattern = "[1-9]{8}")
```
```{r regex6}
str_match_all(c("12345678", "12587465", "dni desconocido"), pattern = "[1-9]{8}")
```


+ `str_replace(string, pattern, replacement)`: reemplaza la primera instancia, `str_replace_all` para reemplazarlas todas
```{r regex7}
str_replace(c("castanya", "otonyo", "veronyo", "anyo", "nyonyo"), pattern = "ny", replacement = "ñ")
```
```{r regex8}
str_replace_all(c("castanya", "otonyo", "veronyo", "anyo", "nyonyo"), pattern = "ny", replacement = "ñ")
```
+ `str_split(string, pattern)`: separa una cadena en un vector, `str_split_fixed(string, pattern, n)` lo hace en un número `n` determinado de elementos

```{r regex9}
print(str_split("Eres muy chu chu chuli",pattern = " "))
print(length(str_split("Eres muy chu chu chuli",pattern = " ")[[1]]))
```

## Analizando texto con R: los Simpsons fuente de sabiduría

### Leer los subtitutos

La librería `subtools` permite leer archivos `.str` y  `.sub`, así como organizar cada diálogo en un data frame.   
Los archivos han de estar organizados en directorios por temporadas y cada uno ha de estar nombrado como `S01xE01` para que se parsee correctamente el número de temporada y de episodio.  

Nos centraremos en las 9 primeras temporadas de los Simpsons. Si descargamos la puntuación y representamos gráficamente el promedio del score por temporada, se observa un claro descenso a partir de la 10. 
Por otro lado son las temporadas que mejor conocemos gracias a sus numerosas repeticiones :)  


```{r, out.width = "150px"}
knitr::include_graphics("./images/ratings.png",dpi = 100)

```



A  continuación leemos los subtítulos ( _en inglés_ ) de las 9 primeras temporadas de los Simpsons.   


```{r subtools, message=FALSE, warning=FALSE, paged.print=FALSE}
#devtools::install_github("fkeck/subtools")

library(subtools)
a <- read.subtitles.serie(dir = "./The Simpsons/")
df <- subDataFrame(a)
df <- df[complete.cases(df), ]
str(df)
```

### Primer análisis: tm

Una primera opción es emplear la librería `tm` para nuestro análisis. La función `tm_map` nos va a permitir preparar nuestor documento para el análisis:

+ conformar un corpus (conjunto estructurado de textos / documentos )
+ transformar en minúsculas nuestro texto
+ eliminar los signos de puntuación
+ eliminar los números
+ eliminar las _stopwords_ 
+ eliminar los espacios en blanco

```{r cleantm, echo=TRUE, message=TRUE, warning=FALSE}
library(tm)
c <- tmCorpus(a)
c <- tm_map(c, content_transformer(tolower))
c <- tm_map(c, removePunctuation)
c <- tm_map(c, removeNumbers)
c <- tm_map(c, removeWords, stopwords("english"))
c <- tm_map(c, stripWhitespace)
c
```

El segundo paso, una vez preparado nuestro corpus, será el de construir la matriz de términos documentos. Para simplificar el análisis cada temporada constituirá un documento. De esta forma obtenemos para cada término la frecuencia por temporada.  

```{r, , message=FALSE, warning=FALSE}
TDM <- TermDocumentMatrix(c)
TDM <- as.matrix(TDM)
vec.season <- c(rep(x = 1,13), rep(2, 22), rep(3,24), rep(4,22), rep(5,22), rep(6,25), rep(7,25),rep(8,25), rep(9,25)) #episodios por temp
TDM.season <- t(apply(TDM, 1, function(x) tapply(x, vec.season, sum)))
colnames(TDM.season) <- paste0("S_", unique(vec.season))
head(TDM.season)
```

A continuación representamos en una nube de términos dichas frecuencias: el tamaño indica el número de repeticiones y el color la temporada en la que más repeticiones presenta. La posición del término respecto de la etiqueta de temporada indica también la frecuencia por temporada.   

```{r, echo=TRUE, message=TRUE, warning=FALSE}
library(wordcloud)
set.seed(100)
comparison.cloud(TDM.season, title.size = 1, max.words = 200, random.order = T)
```

### Text mining con tidytext

La librería tidytext nos permite realizar operaciones como tf (frecuencia de términos) o tf_idf (frecuencia de término - frecuencia inversa de documento) con mayor agilidad.   
  
#### Preparar el corpus

De nuevo vamos a preparar el nuestros diálogos para el análisis eliminado las stopwords.

```{r eliminar stopwords, echo=TRUE, message=FALSE, warning=TRUE}
library(tidytext)
library(tidyverse)

data(stop_words)

tidy_df <- df %>%
  unnest_tokens(word, Text) %>%
  dplyr::anti_join(stop_words)
```

Eliminamos además aquellas "palabras" constituidas exclusivamente por números y la palabra simpson.  

```{r  cleaning, warning=FALSE,echo=TRUE}
library(data.table)
tidy_df <- as.data.table(tidy_df)
tidy_df <- tidy_df[is.na(as.numeric(word))]
tidy_df <- tidy_df[word != 'simpson']
```

#### Term frequency

Con nuestro data set limpio podemos representar en un gráfico de barras la frecuencia de términos. Lo que haremos será:

+ agrupar por temporada
+ sumar el número de veces que aparece el término (`count`)
+ tomar el top 10 por temporada
+ representar en un _barplot_ para cada temporada dicha frecuencia


```{r plot, fig.width=10, message=TRUE, warning=FALSE}
library(ggplot2)
tidy_df %>% group_by(season) %>%
        count(word, sort = FALSE) %>%
        top_n(15) %>%
        ggplot(aes(reorder(word,n), n, fill = season)) +
        geom_col() +
        coord_flip() +
        facet_wrap(~season, scales = "free_y") +
        labs(x = NULL) +
        guides(fill = FALSE) +
        scale_fill_brewer(palette = "Set1")
```

Ahora que disponemos de la frecuencia podemos analizar la evolución en sus apariciones / tramas de cada personaje:
+ calculamos la frecuencia para todos los términos, no solo el top 15
+ creamos dos listas, la familia Simpson y otros personajes relevantes de la trama
+ representamos ambas series series temporales 

```{r, echo=TRUE, fig.width=8, message=FALSE, warning=FALSE}
#install.packages('plotly')
require(plotly)
tidy_tf <- tidy_df %>% group_by(season) %>%
        count(word, sort = TRUE)
tidy_tf <- as.data.table(tidy_tf)

simpson_family <- c('homer', 'bart', 'lisa', 'maggie', 'marge', 'patty','selma')
other_characters <-  c('moe', 'ned', 'barney', 'modd', 'itchy', 'scratchy', 'krusty', 'burns', 'lenny', 'carl', 'edna', 'nelson', 'apu', 'milhouse', 'ralph', 'skinner', 'bob')

myplot <- ggplot(tidy_tf[tidy_tf$word %in% simpson_family], aes(x=season, y=n, group=word)) +
  geom_line(aes(color=word), size=1.25)+
  geom_point(aes(color=word))

ggplotly(myplot)

```

```{r ,echo=TRUE, fig.width=8, message=FALSE, warning=FALSE}
myplot <- ggplot(tidy_tf[tidy_tf$word %in% other_characters], aes(x=season, y=n, group=word)) +
  geom_line(aes(color=word), size=0.75)+
  geom_point(aes(color=word))
ggplotly(myplot)
```

#### Bigramas

También podemos obtener y representar los bigramas (conjunto de 2 términos) y sus frecuencias. Se representan en forma de grafo los más comunes (aquellos que aparecen al menos 7 veces en una misma temporada)

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(igraph)
library(tidytext)

bigram_graph <- df %>%
  unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  group_by(season) %>%
  count(word1, word2, sort = TRUE) %>%
  select(word1, word2, season, n) %>%
  filter(n > 7) %>%
  graph_from_data_frame()
# str(bigram_graph)
```

```{r, echo=TRUE, fig.height=10, fig.width=10, message=TRUE, warning=FALSE}
library(ggraph)
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```
```{r, echo=TRUE, fig.height=10, fig.width=10, message=TRUE, warning=FALSE}
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

####

También podemos realizar tf_idf sobre nuestro corpus para localizar las palabras más relevantes, en general y por temporada

```{r tf_idf, , echo=TRUE, fig.height=10, fig.width=10, message=TRUE, warning=FALSE}
library(dplyr)
tf_idf_df <- tidy_df %>% 
        count(season, word, sort = TRUE) %>%
        bind_tf_idf(word, season, n)
tf_idf_df <- tf_idf_df[order(-tf_idf_df$tf_idf),] 
```

```{r tf_idf2, echo=TRUE, fig.height=10, fig.width=10, message=TRUE, warning=FALSE}
tf_idf_df %>% 
  top_n(20) %>%
  ggplot(aes(word, tf_idf, fill = season)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  coord_flip()
```
```{r byseason, fig.height=8, fig.width=8,echo=TRUE}
tf_idf_df %>% 
  group_by(season) %>% 
  top_n(8) %>% 
  ungroup %>%
  ggplot(aes(reorder(word, tf_idf), tf_idf, fill = season)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~season, ncol = 2, scales = "free") +
  coord_flip()
```

#### Análisis de sentimiento

Valorar como positivo o negativo un mensaje es una tarea compleja, que requiere no sólo conocer el __significado__ de las palabras sino también contextualizarlas, conocer la entonación en que se produce el mensaje, etc. 

En este caso vamos a realizar una aproximación mucho más simple, que es la de considerar el texto como la combinación de palabras individuales y el sentimiento como la suma del sentimiento asociado a cada una de las palabras. Para ello ```tidytext```nos ofrece tres posibles datasets (lexicon) de sentimientos:
+ AFINN from Finn Årup Nielsen (-5, 5),
+ bing from Bing Liu and collaborators("positive" / "negative")
+ nrc from Saif Mohammad and Peter Turney ("yes" / "no").

Todos ellos basados en unigramas. 



```{r sentiment_analysis, echo=TRUE}
head(get_sentiments("afinn"), 3)
```

```{r, echo=TRUE}
head(get_sentiments("bing"), 3)
```
```{r, echo=TRUE}
head(get_sentiments("nrc"), 3)
```

```{r seasonpaste, echo=TRUE}
tidy_df[, season_episode := paste0('S', season_num,"XE", str_pad(episode_num,width = 2,pad = "0"))]
```

```{r bing, echo=TRUE}
library(tidyr)

simpson_sentiment <- tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  count(season_episode, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

head(simpson_sentiment,3)
```
```{r sent, echo=TRUE, fig.height=12, fig.width=12}
simpson_sentiment <- as.data.table(simpson_sentiment)
simpson_sentiment[, season:= str_sub(season_episode,start = 1, end = 2)]
simpson_sentiment[, season:= str_replace(season, "S", "Season ")]
simpson_sentiment[, episode:= str_sub(season_episode,start = 4, end = 6)]

ggplot(simpson_sentiment, aes(episode, sentiment, fill = season)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~season, ncol = 2, scales = "free_x")
```
```{r afinn, echo=TRUE, fig.height=12, fig.width=12}
afinn <- tidy_df %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(season_episode) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")

afinn <- as.data.table(afinn)
afinn[, season:= str_sub(season_episode,start = 1, end = 2)]
afinn[, season:= str_replace(season, "S", "Season ")]
afinn[, episode:= str_sub(season_episode,start = 4, end = 6)]

ggplot(afinn, aes(episode, sentiment, fill = season)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~season, ncol = 2, scales = "free_x")

```



```{r comparativa, echo=TRUE}

resplandior <- tidy_df[season_num==6 & episode_num==6]
afinn_r <- resplandior %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(season_episode) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN") %>% data.table()

bing_and_nrc_r <- bind_rows(resplandior %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          resplandior %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  count(method,  sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)  %>% data.table()

comparativa <- rbind(afinn_r[, c("sentiment", "method"), with = F], bing_and_nrc_r[, c("sentiment", "method"), with = F])


ggplot(data=comparativa, aes(x=method, y=sentiment)) +
  geom_bar(stat="identity",fill="steelblue")+
  theme_minimal()

```



## Anexos y otra información útil

+ Tidy text: http://tidytextmining.com
+ Stringr and Regex: https://rstudio-pubs-static.s3.amazonaws.com/180610_d3764c43f1e54692b7e84d21ec94772a.html
+ Analisis Rick&Morty: http://tamaszilagyi.com/blog/a-tidy-text-analysis-of-rick-and-morty/
+ Ggplot cheatsheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf
+ Plotly cheatsheet: https://images.plot.ly/plotly-documentation/images/r_cheat_sheet.pdf
+ Curso text mining Data camp: https://www.datacamp.com/courses/intro-to-text-mining-bag-of-words
+ Subtools: http://www.pieceofk.fr/?p=437

```{r thankyou, echo=FALSE, out.width="150px"}
knitr::include_graphics("./images/thanks_homer.jpg",dpi = 100)
```
_Si estás leyendo esto en diferido y tienes preguntas no dudes en localizarnos en Twitter_